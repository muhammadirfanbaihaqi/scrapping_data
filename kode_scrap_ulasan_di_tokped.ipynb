{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL produk Tokopedia\n",
    "url = \"https://www.tokopedia.com/xiaomi/official-xiaomi-poco-f6-8gb-256gb-12gb-512gb-snapdragon-8s-gen-3-liquidcool-technology-4-0-iceloop-green-8-256g-fb130?extParam=cmp%3D1%26ivf%3Dfalse%26keyword%3Dhandphone&src=topads\"\n",
    "\n",
    "# Path ke ChromeDriver\n",
    "driver_path = \"C:/Users/DELL/Downloads/chromedriver-win64/chromedriver/chromedriver.exe\"\n",
    "\n",
    "# Buat Service ChromeDriver\n",
    "service = Service(driver_path)\n",
    "\n",
    "# Inisialisasi WebDriver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Buka URL\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Tunggu beberapa detik agar halaman termuat sepenuhnya\n",
    "\n",
    "review_data = []\n",
    "scraped_reviews = set()  # Set untuk menyimpan teks ulasan yang sudah di-scrape\n",
    "max_iterations = 100  # Batas iterasi untuk mencegah loop tak berujung\n",
    "iterations = 0\n",
    "\n",
    "while iterations < max_iterations:\n",
    "    iterations += 1\n",
    "    \n",
    "    # Klik semua tombol \"Selengkapnya\" untuk menampilkan ulasan lengkap\n",
    "    selengkapnya_buttons = driver.find_elements(By.XPATH, \"//button[@class='css-89c2tx']\")\n",
    "    for button in selengkapnya_buttons:\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "        time.sleep(2)  # Tunggu sebentar setelah klik tombol\n",
    "\n",
    "    # Ambil halaman sumber (HTML)\n",
    "    html_source = driver.page_source\n",
    "\n",
    "    # Parsing HTML menggunakan BeautifulSoup\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    # Temukan semua artikel yang berisi ulasan\n",
    "    reviews = soup.find_all('article', class_='css-72zbc4')\n",
    "\n",
    "    # Iterasi melalui setiap ulasan\n",
    "    for review in reviews:\n",
    "        # Ambil rating dari aria-label\n",
    "        rating = review.find('div', {'aria-label': True})\n",
    "        if rating:\n",
    "            rating_value = rating['aria-label']\n",
    "\n",
    "        # Ambil teks ulasan\n",
    "        review_text = review.find('span', {'data-testid': 'lblItemUlasan'}).get_text() if review.find('span', {'data-testid': 'lblItemUlasan'}) else ''\n",
    "\n",
    "        # Tambahkan data ke list jika ulasan belum ada di set\n",
    "        if review_text not in scraped_reviews and review_text != '':\n",
    "            review_data.append({\n",
    "                'Rating': rating_value,\n",
    "                'Review': review_text\n",
    "            })\n",
    "            scraped_reviews.add(review_text)  # Tambahkan teks ulasan ke set jika tidak kosong\n",
    "\n",
    "    try:\n",
    "        # Tunggu hingga tombol halaman berikutnya tersedia dan klik\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[@class='css-16uzo3v-unf-pagination-item' and @aria-label='Laman berikutnya']\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # Tunggu beberapa detik agar halaman berikutnya termuat\n",
    "    except:\n",
    "        break  # Jika tidak ada tombol halaman berikutnya atau tidak bisa diklik, keluar dari loop\n",
    "\n",
    "# Buat DataFrame\n",
    "df = pd.DataFrame(review_data)\n",
    "\n",
    "# Simpan DataFrame ke file Excel\n",
    "df.to_excel('reviewsB5.xlsx', index=False)\n",
    "\n",
    "# Tutup WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWLING / SCRAPPING DATA / MENDAPATKAN DATASET ULASAN PRODUK DARI TOKOPEDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KODE DI ATAS DIGUNAKAN UNTUK MENGAMBIL DATA ULASAN DI TOKPED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di atas melakukan scraping ulasan produk dari halaman Tokopedia menggunakan Selenium dan BeautifulSoup. Pertama, kode menginisialisasi browser dengan ChromeDriver dan membuka URL produk yang diinginkan. Selanjutnya, kode menunggu halaman untuk dimuat dan mengklik semua tombol \"Selengkapnya\" untuk menampilkan teks ulasan lengkap. Setelah itu, kode mengumpulkan data ulasan termasuk rating dan teks ulasan menggunakan BeautifulSoup, memastikan setiap ulasan yang dikumpulkan unik. Kode kemudian mencoba mengklik tombol halaman berikutnya untuk memuat lebih banyak ulasan hingga mencapai batas iterasi. Data ulasan yang berhasil dikumpulkan disimpan dalam DataFrame dan diekspor ke file Excel. Terakhir, browser ditutup untuk mengakhiri sesi scraping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
